{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from gensim.models import word2vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import gensim.sklearn_api\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from emoji import UNICODE_EMOJI\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка датасета, разделение на текстовые данные и таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Судя по датасету, работать предстоит только с признаком \"text\" и таргетом \"airline_sentiment\". Остальные признаки выступают скорее для всевозможных исследовательских работ (определить, как распределены положительные отзывы среди авиакомпаний и т.п.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pd.DataFrame(df.text, columns=['text'])\n",
    "Y = pd.DataFrame(df.airline_sentiment, columns=['airline_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля класса neutral: 21.17%\n",
      "Доля класса positive: 16.14%\n",
      "Доля класса negative: 62.69%\n"
     ]
    }
   ],
   "source": [
    "for i in Y.airline_sentiment.unique():\n",
    "    print('Доля класса {}: {:.2%}'.format(i, len(Y[Y['airline_sentiment']==i]) / len(Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы имеют выраженный дисбаланс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переобозначим классы на -1, 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'negative':-1, 'neutral':0, 'positive':1}\n",
    "Y['airline_sentiment'] = Y['airline_sentiment'].map(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для валидации отложим 10% данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, hold_out_X, y, hold_out_y = train_test_split(texts, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем RandomOverSampler для балансировки классов в датасете X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение классов после RandomOverSampler: [(-1, 8279), (0, 8279), (1, 8279)]\n"
     ]
    }
   ],
   "source": [
    "ros = RandomOverSampler()\n",
    "X, y = ros.fit_resample(X, y)\n",
    "print('Распределение классов после RandomOverSampler: {}'.format(sorted(Counter(y).items())))\n",
    "X = pd.DataFrame(X, columns=['text'])\n",
    "y = pd.DataFrame(y, columns=['airline_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [t for t in X.text]\n",
    "hold_out_tweets = [t for t in hold_out_X.text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Препроцессинг текста твитов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Просмотрев твиты, обнаружил, что практически все записи имеют хештеги, ряд записей имеют ссылки, пунктуацию (в том числе искаженную (!!!! и т.п.), эмодзи). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Две дополнительные функции для обработки текста. Первая соединяет слова no и not со следующим словом через нижнее подчеркивание (чтобы избегать ситуаций, когда эти слова неправильно интерпретируются: not goog/ not bad -> not_good/not_bad).\n",
    "Вторая функция проверяет, является ли токен эмодзи. Во многих случаях эмодзи помогают определить эмоциональный окрас текста/твита."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_preprocessing(texts):\n",
    "    no_prepr = []\n",
    "    for text in texts:\n",
    "        \n",
    "        tmp = ''\n",
    "        \n",
    "        for i in text.split():\n",
    "            if i.lower() == 'no':\n",
    "                tmp += 'no'\n",
    "            elif i.lower() == 'not':\n",
    "                tmp += 'not'\n",
    "            else:\n",
    "                tmp += i + ' '\n",
    "        no_prepr.append(tmp)\n",
    "    \n",
    "    return no_prepr\n",
    "\n",
    "def is_emoji(s):\n",
    "    return s in UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый препроцессинг: (no_preprocessing+len(token)>3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_1(texts):\n",
    "    \n",
    "    mystem = Mystem() \n",
    "    sw = stopwords.words(\"english\")\n",
    "        \n",
    "    extend_list = ['site', 'http', 'https', 'rt', 'rt:']\n",
    "    sw.extend(extend_list)\n",
    "\n",
    "    sw.pop(sw.index('no'))\n",
    "    sw.pop(sw.index('not'))\n",
    "    \n",
    "    clear_tweets = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = re.sub('\\@(\\w+)', '', text) # удаление названий авиакомпаний\n",
    "        text = re.sub('#(\\w+)', \" \", text) # удаление хештегов\n",
    "        text = re.sub(\"[^a-zA-Z,_]\", \" \", text) # удаление лишних спецсимволов\n",
    "        text = text.strip(\" \")\n",
    "        \n",
    "        tokens = mystem.lemmatize(text.lower())\n",
    "        tokens = [token for token in tokens if token not in sw \n",
    "                  and token != \" \" \n",
    "                  and token.strip() not in punctuation \n",
    "                  and len(token)>1]\n",
    "        \n",
    "        text = \" \".join(tokens)\n",
    "        \n",
    "        clear_tweets.append(text)\n",
    "        \n",
    "    return no_preprocessing(clear_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Второй препроцессинг: (len(token)>2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_2(texts):\n",
    "    \n",
    "    mystem = Mystem() \n",
    "    sw = stopwords.words(\"english\")\n",
    "        \n",
    "    extend_list = ['site', 'http', 'https', 'rt', 'rt:']\n",
    "    sw.extend(extend_list)\n",
    "\n",
    "    clear_tweets = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = re.sub('\\@(\\w+)', '', text) # удаление названий авиакомпаний\n",
    "        text = re.sub('#(\\w+)', \" \", text) # удаление хештегов\n",
    "        text = re.sub(\"[^a-zA-Z,_]\", \" \", text) # удаление лишних спецсимволов\n",
    "        text = text.strip(\" \")\n",
    "        \n",
    "        tokens = mystem.lemmatize(text.lower())\n",
    "        tokens = [token for token in tokens if token not in sw \n",
    "                  and token != \" \" \n",
    "                  and token.strip() not in punctuation \n",
    "                  and len(token)>2]\n",
    "        \n",
    "        text = \" \".join(tokens)\n",
    "        \n",
    "        clear_tweets.append(text)\n",
    "        \n",
    "    return clear_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Третий препроцессинг: (no_preprocessing+is_emoji+len(token)>2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_3(texts):\n",
    "    \n",
    "    mystem = Mystem() \n",
    "    sw = stopwords.words(\"english\")\n",
    "        \n",
    "    extend_list = ['site', 'http', 'https', 'rt', 'rt:']\n",
    "    sw.extend(extend_list)\n",
    "\n",
    "    sw.pop(sw.index('no'))\n",
    "    sw.pop(sw.index('not'))\n",
    "    \n",
    "    clear_tweets = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = re.sub('\\@(\\w+)', '', text) # удаление названий авиакомпаний\n",
    "        text = re.sub('#(\\w+)', \" \", text) # удаление хештегов\n",
    "        text = re.sub('[#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\"1234567890]', \" \", text) #удаление шума\n",
    "        text = text.strip(\" \")\n",
    "        \n",
    "        tokens = mystem.lemmatize(text.lower())\n",
    "        tokens = [token for token in tokens if token not in sw \n",
    "                  and token != \" \" \n",
    "                  and token.strip() not in punctuation \n",
    "                  and (len(token)>2 or is_emoji(token))]\n",
    "        \n",
    "        text = \" \".join(tokens)\n",
    "        \n",
    "        clear_tweets.append(text)\n",
    "        \n",
    "    return no_preprocessing(clear_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Четвертый препроцессинг: (is_emoji+len(token)>2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_4(texts):\n",
    "    \n",
    "    mystem = Mystem() \n",
    "    sw = stopwords.words(\"english\")\n",
    "        \n",
    "    extend_list = ['site', 'http', 'https', 'rt', 'rt:']\n",
    "    sw.extend(extend_list)\n",
    "    \n",
    "    clear_tweets = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = re.sub('\\@(\\w+)', '', text) # удаление названий авиакомпаний\n",
    "        text = re.sub('#(\\w+)', \" \", text) # удаление хештегов\n",
    "        text = re.sub('[#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\"1234567890]', \" \", text) #удаление шума\n",
    "        text = text.strip(\" \")\n",
    "        \n",
    "        tokens = mystem.lemmatize(text.lower())\n",
    "        tokens = [token for token in tokens if token not in sw \n",
    "                  and token != \" \" \n",
    "                  and token.strip() not in punctuation \n",
    "                  and (len(token)>2 or is_emoji(token))]\n",
    "        \n",
    "        text = \" \".join(tokens)\n",
    "        \n",
    "        clear_tweets.append(text)\n",
    "        \n",
    "    return clear_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные с учетом препроцессинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_tweets_for_train = [('no_prepr', tweets), ('prepr_train_1', prep_1(tweets)), \n",
    "                               ('prepr_train_2', prep_2(tweets)), ('prepr_train_3', prep_3(tweets)), \n",
    "                               ('prepr_train_4', prep_4(tweets))]\n",
    "\n",
    "preprocess_hold_out_tweets = [('no_prepr', hold_out_tweets), ('prep_hold_out_1', prep_1(hold_out_tweets)), ('prep_hold_out_2', prep_2(hold_out_tweets)),\n",
    "                               ('prep_hold_out_3', prep_3(hold_out_tweets)), ('prep_hold_out_4', prep_4(hold_out_tweets))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Преобразование признаков и обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учитывая, что твиты короткие, будем использовать n-граммы (биграммы/триграммы). Посмотрим на качество работы классификаторов на кросс-валидации, а также на отложенной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "classifiers = [('LogisticRegression', LogisticRegression(class_weight='balanced')), \n",
    "              ('SGDClassifier', SGDClassifier(class_weight='balanced')), \n",
    "              ('LinearSVC', LinearSVC(class_weight='balanced')), \n",
    "              ('XGBClassifier', XGBClassifier(n_estimators=100, learning_rate=0.5, n_jobs=-1)),\n",
    "              ('RandomForestClassifier', RandomForestClassifier(class_weight='balanced', n_estimators=100)),\n",
    "              ('MultinomialNB', MultinomialNB()),\n",
    "              ('BernoulliNB', BernoulliNB()),\n",
    "              ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=3))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_cvs_cv = []\n",
    "scores_ho = []\n",
    "\n",
    "for c in classifiers:\n",
    "    \n",
    "    pipeline = Pipeline([('Count_vectorizer', c_vectorizer), c])\n",
    "    \n",
    "    for pt in preprocess_tweets_for_train:\n",
    "        cvs = cross_val_score(pipeline, pt[1], y, cv=4)\n",
    "        scores_cvs_cv.append((c[0], pt[0], cvs.mean()))\n",
    "        pipeline.fit(pt[1], y)\n",
    "        \n",
    "    for ph in preprocess_hold_out_tweets:\n",
    "        pred = pipeline.predict(ph[1])\n",
    "        acc_holdout = accuracy_score(hold_out_y, pred)\n",
    "        scores_ho.append((c[0], ph[0], acc_holdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_cvs_cv = sorted(scores_cvs_cv, key=lambda x: x[2], reverse=True)\n",
    "scores_ho = sorted(scores_ho, key=lambda x: x[2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SGDClassifier', 'no_prepr', 0.9493126167205883),\n",
       " ('RandomForestClassifier', 'no_prepr', 0.9468085647704542),\n",
       " ('MultinomialNB', 'no_prepr', 0.9436176344690067),\n",
       " ('LinearSVC', 'no_prepr', 0.9409112002915901),\n",
       " ('LogisticRegression', 'no_prepr', 0.9407899392645868),\n",
       " ('SGDClassifier', 'prepr_train_4', 0.9310159990969223),\n",
       " ('SGDClassifier', 'prepr_train_1', 0.9301271886480265),\n",
       " ('SGDClassifier', 'prepr_train_3', 0.929238769616261),\n",
       " ('SGDClassifier', 'prepr_train_2', 0.9281882256090764),\n",
       " ('LogisticRegression', 'prepr_train_1', 0.9190605738363012)]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_cvs_cv[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SGDClassifier', 'no_prepr', 0.7903005464480874),\n",
       " ('SGDClassifier', 'prep_hold_out_2', 0.7868852459016393),\n",
       " ('SGDClassifier', 'prep_hold_out_4', 0.7868852459016393),\n",
       " ('SGDClassifier', 'prep_hold_out_3', 0.782103825136612),\n",
       " ('SGDClassifier', 'prep_hold_out_1', 0.7786885245901639),\n",
       " ('MultinomialNB', 'prep_hold_out_2', 0.7780054644808743),\n",
       " ('MultinomialNB', 'prep_hold_out_3', 0.7780054644808743),\n",
       " ('MultinomialNB', 'prep_hold_out_4', 0.7780054644808743),\n",
       " ('LogisticRegression', 'prep_hold_out_2', 0.7773224043715847),\n",
       " ('LogisticRegression', 'prep_hold_out_4', 0.7773224043715847)]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_ho[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходя из результатов можно сделать следующие выводы:\n",
    "\n",
    "1. На трейне имеем очень высокую точность по сравнению с отложенной выборкой = переобучение\n",
    "2. Препроцессинг в некоторых классификаторах оказывает влияние на качество его работы, однако практически везде разница незначительна.\n",
    "3. Для дальнейшего поиска параметров по сетке будем использовать классификаторы, показавшие лучшее качество на отложенной выборке = LogisticRegression и SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 36 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   26.4s\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.94079\n",
      "Best parameters set for SGDClassifier:\n",
      "max_df = 0.5\n",
      "ngram_range = (1, 3)\n",
      "alpha = 0.001\n",
      "penalty = l2\n"
     ]
    }
   ],
   "source": [
    "pipeline_cv_lr = Pipeline([('CountVectorizer', CountVectorizer()), \n",
    "                            ('clf', LogisticRegression(class_weight='balanced'))])\n",
    "\n",
    "parameters = {\n",
    "    'CountVectorizer__max_df': (0.5, 0.75, 1.0),\n",
    "    'CountVectorizer__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
    "    'clf__tol': [0.001, 0.0001],\n",
    "    'clf__penalty': ['l2', 'l1']}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline_cv_lr, parameters, cv=4, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(preprocess_tweets_for_train[0][1], y)\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "print(\"Best score: %0.5f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set for SGDClassifier:\")\n",
    "print('max_df =', best_parameters['CountVectorizer__max_df'])\n",
    "print('ngram_range =', best_parameters['CountVectorizer__ngram_range'])\n",
    "print('alpha =', best_parameters['clf__tol'])\n",
    "print('penalty =', best_parameters['clf__penalty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8121584699453552"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_cv_lr = Pipeline([('CountVectorizer', CountVectorizer(max_df=0.5, ngram_range=(1, 3))), \n",
    "                            ('clf', LogisticRegression(class_weight='balanced'))])\n",
    "pipeline_cv_lr.fit(tweets, y)\n",
    "accuracy_score(hold_out_y, pipeline_cv_lr.predict(hold_out_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 81 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 324 out of 324 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.95210\n",
      "Best parameters set for SGDClassifier:\n",
      "max_df = 0.5\n",
      "ngram_range = (1, 3)\n",
      "alpha = 1e-05\n",
      "penalty = None\n"
     ]
    }
   ],
   "source": [
    "pipeline_cv_sgd = Pipeline([('CountVectorizer', CountVectorizer()), \n",
    "                            ('clf', SGDClassifier(class_weight='balanced'))])\n",
    "\n",
    "parameters = {\n",
    "    'CountVectorizer__max_df': (0.5, 0.75, 1.0),\n",
    "    'CountVectorizer__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
    "    'clf__alpha': [0.001, 0.0001, 0.00001],\n",
    "    'clf__penalty': [None, 'l2', 'l1']}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline_cv_sgd, parameters, cv=4, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(preprocess_tweets_for_train[0][1], y)\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "print(\"Best score: %0.5f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set for SGDClassifier:\")\n",
    "print('max_df =', best_parameters['CountVectorizer__max_df'])\n",
    "print('ngram_range =', best_parameters['CountVectorizer__ngram_range'])\n",
    "print('alpha =', best_parameters['clf__alpha'])\n",
    "print('penalty =', best_parameters['clf__penalty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8121584699453552"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_cv_sgd = Pipeline([('CountVectorizer', CountVectorizer(max_df=0.5, ngram_range=(1,3))), \n",
    "                     ('clf', SGDClassifier(class_weight='balanced'))])\n",
    "pipeline_cv_sgd.fit(tweets, y)\n",
    "accuracy_score(hold_out_y, pipeline_cv_sgd.predict(hold_out_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате получили два пайплайна с точностью около 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidif vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Повторим на тех же классификаторах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_vectorizer = TfidfVectorizer(ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_cvs_tv = []\n",
    "scores_ho_tv = []\n",
    "\n",
    "for c in classifiers:\n",
    "    \n",
    "    pipeline = Pipeline([('Tfidf_vectorizer', t_vectorizer), c])\n",
    "    \n",
    "    for pt in preprocess_tweets_for_train:\n",
    "        cvs = cross_val_score(pipeline, pt[1], y, cv=4)\n",
    "        scores_cvs_tv.append((c[0], pt[0], cvs.mean()))\n",
    "        pipeline.fit(pt[1], y)\n",
    "        \n",
    "    for ph in preprocess_hold_out_tweets:\n",
    "        pred = pipeline.predict(ph[1])\n",
    "        acc_holdout = accuracy_score(hold_out_y, pred)\n",
    "        scores_ho_tv.append((c[0], ph[0], acc_holdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_cvs_tv = sorted(scores_cvs_tv, key=lambda x: x[2], reverse=True)\n",
    "scores_ho_tv = sorted(scores_ho_tv, key=lambda x: x[2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LinearSVC', 'no_prepr', 0.9504836584913718),\n",
       " ('LinearSVC', 'prepr_train_4', 0.9367110204902171),\n",
       " ('LinearSVC', 'prepr_train_2', 0.936670626242358),\n",
       " ('LinearSVC', 'prepr_train_3', 0.9359840218830359),\n",
       " ('LinearSVC', 'prepr_train_1', 0.9356205127940169)]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_cvs_tv[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SGDClassifier', 'prep_hold_out_2', 0.7896174863387978),\n",
       " ('SGDClassifier', 'prep_hold_out_4', 0.7896174863387978),\n",
       " ('SGDClassifier', 'prep_hold_out_3', 0.787568306010929),\n",
       " ('SGDClassifier', 'prep_hold_out_1', 0.7868852459016393),\n",
       " ('LinearSVC', 'prep_hold_out_2', 0.7862021857923497),\n",
       " ('LinearSVC', 'prep_hold_out_4', 0.7862021857923497),\n",
       " ('LogisticRegression', 'prep_hold_out_2', 0.7814207650273224),\n",
       " ('LogisticRegression', 'prep_hold_out_4', 0.7814207650273224),\n",
       " ('LinearSVC', 'prep_hold_out_3', 0.7814207650273224),\n",
       " ('LogisticRegression', 'prep_hold_out_1', 0.7800546448087432)]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_ho_tv[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 81 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 324 out of 324 | elapsed:  4.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.95335\n",
      "Best parameters set for LinearSVC:\n",
      "max_df = 0.5\n",
      "ngram_range = (1, 3)\n",
      "tol = 0.001\n",
      "C = 10\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('TfidfVectorizer', TfidfVectorizer()), ('clf', LinearSVC(class_weight='balanced'))])\n",
    "\n",
    "parameters = {\n",
    "    'TfidfVectorizer__max_df': (0.5, 0.25, 1.0),\n",
    "    'TfidfVectorizer__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
    "    'clf__tol': [0.001, 0.0001, 0.00001],\n",
    "    'clf__C': [0.1, 1, 10]}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=4, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(preprocess_tweets_for_train[0][1], y)\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "print(\"Best score: %0.5f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set for LinearSVC:\")\n",
    "print('max_df =', best_parameters['TfidfVectorizer__max_df'])\n",
    "print('ngram_range =', best_parameters['TfidfVectorizer__ngram_range'])\n",
    "print('tol =', best_parameters['clf__tol'])\n",
    "print('C =', best_parameters['clf__C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8046448087431693"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_tv_lsvc = Pipeline([('CountVectorizer', CountVectorizer(max_df=0.5, ngram_range=(1,3))), \n",
    "                     ('clf', LinearSVC(class_weight='balanced', tol=0.001, C=10))])\n",
    "pipeline_tv_lsvc.fit(tweets, y)\n",
    "accuracy_score(hold_out_y, pipeline_tv_lsvc.predict(hold_out_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 72 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 288 out of 288 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.95319\n",
      "Best parameters set for LinearSVC:\n",
      "max_df = 0.75\n",
      "ngram_range = (1, 3)\n",
      "alpha = 1e-05\n",
      "penalty = l2\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('TfidfVectorizer', TfidfVectorizer()), ('clf', SGDClassifier(class_weight='balanced'))])\n",
    "\n",
    "parameters = {\n",
    "    'TfidfVectorizer__max_df': (0.25, 0.5, 0.75, 1.0),\n",
    "    'TfidfVectorizer__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
    "    'clf__alpha': [0.001, 0.0001, 0.00001],\n",
    "    'clf__penalty': ['l2', 'l1']}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=4, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(preprocess_tweets_for_train[0][1], y)\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "print(\"Best score: %0.5f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set for LinearSVC:\")\n",
    "print('max_df =', best_parameters['TfidfVectorizer__max_df'])\n",
    "print('ngram_range =', best_parameters['TfidfVectorizer__ngram_range'])\n",
    "print('alpha =', best_parameters['clf__alpha'])\n",
    "print('penalty =', best_parameters['clf__penalty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8080601092896175"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_tv_sgd = Pipeline([('TfidfVectorizer', TfidfVectorizer(ngram_range=(1,3), max_df=1.0)), \n",
    "                     ('clf', SGDClassifier(class_weight='balanced', alpha=1e-05))])\n",
    "pipeline_tv_sgd.fit(tweets, y)\n",
    "accuracy_score(hold_out_y, pipeline_tv_sgd.predict(hold_out_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пайплайны с TfidfVectorizer отработали сравнимо по качеству с пайплайнами с CountVectorizer. Пока лучшее качество у TfidfVectorizer+SGDClassifier (0.812)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим словарь word2vec и создадим класс с функциями fit и transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_list = [i.split() for i in preprocess_tweets_for_train[0][1]]\n",
    "model = word2vec.Word2Vec(min_count=3)\n",
    "model.build_vocab(ct_list)\n",
    "model.train(ct_list, total_examples=model.corpus_count, epochs=model.iter)\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим отдельный класс с функциями fit, transform для w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanVect(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(word2vec.values())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(10)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать следующие классификаторы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers2 = [('LogisticRegression', LogisticRegression(class_weight='balanced')), \n",
    "              ('SGDClassifier', SGDClassifier(class_weight='balanced')), \n",
    "              ('LinearSVC', LinearSVC(class_weight='balanced')), \n",
    "              ('XGBClassifier', XGBClassifier(n_estimators=100, learning_rate=0.5, n_jobs=-1)),\n",
    "              ('RandomForestClassifier', RandomForestClassifier(class_weight='balanced', n_estimators=100, n_jobs=-1)),\n",
    "              ('BernoulliNB', BernoulliNB())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим качество на кросс-валидации и на отложенной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvs: LogisticRegression 0.50785570266577\n",
      "ho: 0.5218579234972678\n",
      "cvs: SGDClassifier 0.46023276324211126\n",
      "ho: 0.3025956284153005\n",
      "cvs: LinearSVC 0.5421863316198657\n",
      "ho: 0.5594262295081968\n",
      "cvs: XGBClassifier 0.7005540548622775\n",
      "ho: 0.5689890710382514\n",
      "cvs: RandomForestClassifier 0.9211618184175225\n",
      "ho: 0.6468579234972678\n",
      "cvs: BernoulliNB 0.42089761185449054\n",
      "ho: 0.5594262295081968\n"
     ]
    }
   ],
   "source": [
    "for c in classifiers2:\n",
    "    \n",
    "    pipeline = Pipeline([(\"word2vec\", MeanVect(w2v)), c])\n",
    "    \n",
    "    cvs = cross_val_score(pipeline, preprocess_tweets_for_train[0][1], y, cv=4)\n",
    "    print('cvs:', c[0], cvs.mean())\n",
    "    pipeline.fit(preprocess_tweets_for_train[0][1], y)\n",
    "        \n",
    "    pred = pipeline.predict(hold_out_tweets)\n",
    "    acc_holdout = accuracy_score(hold_out_y, pred)\n",
    "    print('ho:', acc_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество на моделях с Word2vec оказалось значительно ниже, чем на пайплайнах ранее - самый высокий показатель accuracy на пайплайне word2vec+RandomForestClassifier (0.636), поэтому дополнительные процедуры (поиск по сетке, прогон на других данных (другие препроцессинги) и тд) не целесообразны (по крайней мере в той постановке задачи, которая решается)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vowpal wabbit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим классы так, чтобы не было отрицательных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = X.text\n",
    "topic_encoder = LabelEncoder()\n",
    "all_targets_mult = topic_encoder.fit_transform(y.airline_sentiment) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим на train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents, test_documents, train_labels_mult, test_labels_mult = \\\n",
    "    train_test_split(all_documents, all_targets_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для подгонки данных к формату vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vw_format(document, label=None):\n",
    "    return str(label or '') + ' |text ' + ' '.join(re.findall('\\w{3,}', document.lower())) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.vw', 'w') as vw_train_data:\n",
    "    for text, target in zip(train_documents, train_labels_mult):\n",
    "        vw_train_data.write(to_vw_format(text, target))\n",
    "with open('test.vw', 'w') as vw_test_data:\n",
    "    for text in test_documents:\n",
    "        vw_test_data.write(to_vw_format(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One against all на 3 класса с функцией потерь hinge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = model.vw\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        3        1        6\n",
      "1.000000 1.000000            2            2.0        1        3        7\n",
      "0.750000 0.500000            4            4.0        1        1       19\n",
      "0.750000 0.750000            8            8.0        3        1       10\n",
      "0.687500 0.625000           16           16.0        2        3        9\n",
      "0.656250 0.625000           32           32.0        3        3        4\n",
      "0.625000 0.593750           64           64.0        1        3       17\n",
      "0.539062 0.453125          128          128.0        1        1       14\n",
      "0.496094 0.453125          256          256.0        1        3       18\n",
      "0.423828 0.351562          512          512.0        3        2       13\n",
      "0.364258 0.304688         1024         1024.0        3        3       21\n",
      "0.327148 0.290039         2048         2048.0        3        3        9\n",
      "0.279053 0.230957         4096         4096.0        1        1       17\n",
      "0.244507 0.209961         8192         8192.0        3        3        6\n",
      "0.207642 0.170776        16384        16384.0        1        2       11\n",
      "\n",
      "finished run\n",
      "number of examples = 18569\n",
      "weighted example sum = 18569.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.201788\n",
      "total feature number = 251113\n",
      "CPU times: user 4.88 ms, sys: 19.9 ms, total: 24.8 ms\n",
      "Wall time: 201 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw --oaa 3 train.vw -f model.vw --loss_function=hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "predictions = predictions.txt\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = test.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "    n.a.     n.a.            1            1.0  unknown        3       10\n",
      "    n.a.     n.a.            2            2.0  unknown        1        9\n",
      "    n.a.     n.a.            4            4.0  unknown        3        5\n",
      "    n.a.     n.a.            8            8.0  unknown        1       23\n",
      "    n.a.     n.a.           16           16.0  unknown        3       19\n",
      "    n.a.     n.a.           32           32.0  unknown        2       12\n",
      "    n.a.     n.a.           64           64.0  unknown        1       19\n",
      "    n.a.     n.a.          128          128.0  unknown        1       21\n",
      "    n.a.     n.a.          256          256.0  unknown        2       25\n",
      "    n.a.     n.a.          512          512.0  unknown        2        8\n",
      "    n.a.     n.a.         1024         1024.0  unknown        2        6\n",
      "    n.a.     n.a.         2048         2048.0  unknown        2       18\n",
      "    n.a.     n.a.         4096         4096.0  unknown        3       15\n",
      "\n",
      "finished run\n",
      "number of examples = 6190\n",
      "weighted example sum = 6190.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = n.a.\n",
      "total feature number = 84768\n",
      "CPU times: user 4.36 ms, sys: 15 ms, total: 19.4 ms\n",
      "Wall time: 171 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -i model.vw -t -d test.vw -p predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predictions.txt') as pred_file:\n",
    "    test_prediction_mult = [float(label) \n",
    "                            for label in pred_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8473344103392568"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_labels_mult, test_prediction_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили высокий показатель точности, в сравнении с прошлыми моделями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.text, df.airline_sentiment, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем токенизацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(num_words=10000)\n",
    "tk.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tk.texts_to_sequences(X_train)\n",
    "X_test_seq = tk.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция создает матрицу нулей, после чего записывает на соответствующие позиции значения токенизированных текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_seq(seqs, nb_features = 10000):\n",
    "    ohs = np.zeros((len(seqs), nb_features))\n",
    "    for i, s in enumerate(seqs):\n",
    "        ohs[i, s] = 1.0\n",
    "    return ohs\n",
    "\n",
    "X_train_oh = one_hot_seq(X_train_seq)\n",
    "X_test_oh = one_hot_seq(X_test_seq)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_le = le.fit_transform(y_train)\n",
    "y_test_le = le.transform(y_test)\n",
    "y_train_oh = to_categorical(y_train_le)\n",
    "y_test_oh = to_categorical(y_test_le)\n",
    "\n",
    "X_train_rest, X_valid, y_train_rest, y_valid = train_test_split(X_train_oh, y_train_oh, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем нейросеть с одним скрытым слоем, активационными функциями relu на входном и скрытом слое, softmax на выходном. Также, добавим дропаут против переобучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(128, init = 'uniform', activation='relu', input_shape=(10000,)))\n",
    "nn.add(layers.Dropout(0.6))\n",
    "nn.add(layers.Dense(128,init = 'uniform', activation='relu'))\n",
    "nn.add(layers.Dropout(0.6))\n",
    "nn.add(layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве функции потерь будем использовать кроссэнтропию. Прогоним на 10 эпохах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11858/11858 [==============================] - 3s 259us/step - loss: 0.4787 - acc: 0.7760\n",
      "Epoch 2/10\n",
      "11858/11858 [==============================] - 2s 134us/step - loss: 0.3583 - acc: 0.8374\n",
      "Epoch 3/10\n",
      "11858/11858 [==============================] - 2s 136us/step - loss: 0.2690 - acc: 0.8884\n",
      "Epoch 4/10\n",
      "11858/11858 [==============================] - 2s 134us/step - loss: 0.2124 - acc: 0.9148\n",
      "Epoch 5/10\n",
      "11858/11858 [==============================] - 2s 136us/step - loss: 0.1665 - acc: 0.9375\n",
      "Epoch 6/10\n",
      "11858/11858 [==============================] - 2s 134us/step - loss: 0.1377 - acc: 0.9475\n",
      "Epoch 7/10\n",
      "11858/11858 [==============================] - 2s 133us/step - loss: 0.1123 - acc: 0.9598\n",
      "Epoch 8/10\n",
      "11858/11858 [==============================] - 2s 133us/step - loss: 0.0905 - acc: 0.9686\n",
      "Epoch 9/10\n",
      "11858/11858 [==============================] - 2s 133us/step - loss: 0.0795 - acc: 0.9723\n",
      "Epoch 10/10\n",
      "11858/11858 [==============================] - 2s 135us/step - loss: 0.0649 - acc: 0.9779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1abaa96e48>"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "nn.fit(X_train_rest,y_train_rest, batch_size = 128, nb_epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = nn.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учитывая, что получаем вероятностную оценку, в качестве метрики будем использовать не accuracy, а roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9160354078458003"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_valid, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы по проделанной работе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Загрузили датасет, провели анализ распределения целевой переменной (выявлен дисбаланс классов), отложили 10% данных для проверки качества работы.\n",
    "\n",
    "2. Сделали oversampling выборки чтобы сбалансировать классы.\n",
    "\n",
    "3. Привели 4 возможных препроцессинга.\n",
    "\n",
    "4. На 8 алгоритмах классификации и 5 наборах данных (4 препроцессинга+без обработки) провели оценку качества на кросс-валидации и отложенной выборке с использованием CountVectorizer и TfidfVectorizer. Для двух лучших алгоритмов каждого векторайзера провели поиск по сетке. Accuracy порядка 0.79-0.81.\n",
    "\n",
    "5. Word2vec показал относительно низкое качество - пайплайн с Rf показал accuracy порядка 0.64. На остальных классификаторах accuracy оказался еще ниже. Дальнейшее рассмотрение word2vec опустили. Это связано с тем, что тексты твитов весьма короткие. На больших текстах Word2vec (ну и с достаточным объемом выборки) обычно показывает лучшее качество.\n",
    "\n",
    "6. Vowpal wabbit показал accuracy порядка 0.85. Изменение параметров алгоритма не улучшало этот показатель.\n",
    "\n",
    "7. Нейронная сеть с одним скрытым слоем показала accuracy на отложенной выборке порядка 0.9, что является лучшим показателем из всех ранее рассмотренных моделей.\n",
    "\n",
    "8. Как еще повысить качество моделей: стекинг алгоритмов, дополнительные данные (например, напарсить новые с использованием beautifulsoup/scrapy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
